{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMidHFS2Hc9fMZHNMRjhxr5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y38JRrTfGZff"
      },
      "outputs": [],
      "source": [
        "# transformer 라이브러리 설치\n",
        "!pip install transformers\n",
        "\n",
        "# 라이브러리\n",
        "from transformers import pipeline\n",
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')  # Tokenizer 객체 생성\n",
        "model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization') # 모델 객체 생성\n",
        "\n",
        "vocab = tokenizer.get_vocab()                                 # 단어 사전 생성\n",
        "'''\n",
        "print(tokenizer.all_special_tokens)                           # tokenizer의 special token\n",
        "print(tokenizer.all_special_ids)\n",
        "print(tokenizer.bos_token_id)\n",
        "print(tokenizer.eos_token_id)\n",
        "'''\n",
        "\n",
        "# 한글 텍스트\n",
        "text = '''\n",
        "####\n",
        "'''\n",
        "\n",
        "text_kor = text.replace('\\n', ' ')                                            # 데이터 전처리\n",
        "input_ids = tokenizer.encode(text_kor)                                        # 토큰화+수치화\n",
        "input_ids = [tokenizer.bos_token_id] + input_ids + [tokenizer.eos_token_id]   # 입력 데이터 생성\n",
        "print(input_ids)\n",
        "\n",
        "# 요약 문장 생성\n",
        "import torch\n",
        "\n",
        "sentence = torch.tensor([input_ids])\n",
        "summary_ids = model.generate(sentence, num_beams = 4, max_length = 512)\n",
        "\n",
        "# 디코딩된 요약 문장 한글로 복원\n",
        "summarized_text = tokenizer.decode(summary_ids[0].tolist(),\n",
        "                                   skip_special_tokens = True)\n",
        "print(summarized_text)\n",
        "\n",
        "# 토큰화+수치화\n",
        "input_ids = tokenizer.encode(text_kor)\n",
        "\n",
        "# 입력 데이터 생성\n",
        "input_ids = [tokenizer.bos_token_id] + input_ids + [tokenizer.eos_token_id]\n",
        "print(input_ids)\n"
      ]
    }
  ]
}