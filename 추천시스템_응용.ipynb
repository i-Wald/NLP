{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuei1J6-lQsU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive                                # 구글 드라이버와 연동\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import pandas as pd                                            # 데이터프레임 형성\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel             # 여러 문장 유사도 계산\n",
        "\n",
        "file = \"/content/drive/MyDrive/NLP/movies_metadata.csv\"        # 활용 데이터\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.read_csv(file)                                         # csv 파일 읽기\n",
        "pd.options.display.max_columns = None                          # 모든 컬럼 보이기\n",
        "print(f'상위 5개의 데이터: \\n{df.head()}')                     # 부분 데이터 출력\n",
        "\n",
        "# 컬럼의 결측치 확인\n",
        "overview_null = df['overview'].isnull().sum()\n",
        "title_null = df[\"title\"].isnull().sum()\n",
        "print(f'줄거리 컬럼의 결측치 개수: {overview_null}')\n",
        "print(f'제목 컬럼의 결측치 개수: {title_null}')\n",
        "\n",
        "# 컬럼의 결측치 제거\n",
        "df.dropna(subset = [\"overview\", \"title\"], inplace = True)\n",
        "\n",
        "# 불용어 제거(영어)\n",
        "tfidf = TfidfVectorizer(stop_words = 'english')\n",
        "\n",
        "# 단어 사전 생성\n",
        "data = df.iloc[:20000, :]                                        # 부분 활용 데이터(2만개)\n",
        "tfidf.fit(data.overview)                                        # 단어 사전 생성(줄거리)\n",
        "vocab = tfidf.vocabulary_\n",
        "print(f'2만 개 영화 줄거리 단어의 개수: {len(vocab)}')          # 단어 사전 개수 확인\n",
        "\n",
        "matrix = tfidf.transform(data.overview).toarray()               # 각 문장의 행렬 변환\n",
        "print(f'행렬 모양: {matrix.shape}')                             # 행렬 모양 확인\n",
        "\n",
        "# 인덱스 재설정\n",
        "reset_data = data.reset_index(drop = True)\n",
        "\n",
        "# 유사도 측정\n",
        "similar_matrix = linear_kernel(matrix, matrix)\n",
        "print(f'유사도 측정: \\n{similar_matrix}')\n",
        "\n",
        "# 제목 순서화(인덱스) + 중복 제목 제거\n",
        "extract_index = pd.Series(data = reset_data.index, index = reset_data.title).drop_duplicates()\n",
        "\n",
        "# 영화 추천 함수생성\n",
        "def recommend(title, similar_matrix):\n",
        "\n",
        "    # 영화의 제목을 이용해 해당 영화의 인덱스 확인\n",
        "    idx_title = extract_index[title]\n",
        "\n",
        "    # 모든 영화와 특정 영화와의 유사도 구하기\n",
        "    similar_score = [(idx, score) for idx, score in enumerate(similar_matrix[idx_title])]\n",
        "    print(f'유사도 측정 결과: \\n{similar_score}')\n",
        "\n",
        "    # 유사도에 따른 정렬\n",
        "    sorted_similar_score = sorted(similar_score, key = lambda x:x[1], reverse = True)\n",
        "    print(sorted_similar_score)\n",
        "\n",
        "    # 유사도가 가장 큰 10개의 영화의 인덱스 확인\n",
        "    similar_scores = sorted_similar_score[1:11]\n",
        "    print(f'유사도가 높은 10개의 영화: \\n{similar_scores}')\n",
        "    movie_index = [data[0] for data in similar_scores]\n",
        "    print(f'유사도가 높은 10개 영화의 인덱스: \\n{movie_index}')\n",
        "\n",
        "    # 유사도가 가장 큰 10개의 영화 제목 확인\n",
        "    movie_title = data['title'].iloc[movie_index]\n",
        "\n",
        "    return movie_title\n",
        "\n",
        "# 함수 테스트\n",
        "recommended_movies = recommend(\"Jumanji\", similar_matrix)\n",
        "print(f'Jumanji와 유사도가 높은 10개의 영화: \\n{recommended_movies}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/8vecF0kmAWCkE8GH1Hkx"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
