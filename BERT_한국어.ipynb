{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfph/c6K3ykkS8vYygJOI3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lohgstXKUAbD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers              # hugging faces 활용\n",
        "!pip install tqdm                      # 진행과정 보기\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "np.random.seed(43)\n",
        "tf.random.set_seed(43)\n",
        "\n",
        "# 한국어 Bert Tokenizer\n",
        "klue_tokenizer = BertTokenizer.from_pretrained('klue/bert-base',\n",
        "                                               cache_dir = 'bert-ckpt',\n",
        "                                               do_lower_case = False)\n",
        "\n",
        "# 학습 데이터\n",
        "file_path = '/content/drive/MyDrive/NLP/ratings_train.txt'\n",
        "train_data = pd.read_csv(file_path, sep = '\\t', quoting = 3)\n",
        "train_data.dropna(inplace=True)           #결측치 제거\n",
        "train_data_counts = train_data.document.apply(lambda x: len(tokenizer.tokenize(x)))\n",
        "print(np.quantile(train_data_counts, .75)) # 토큰 개수의 75% 해당 값\n",
        "\n",
        "# 모델 학습\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "valid_split = 0.2\n",
        "max_len = 25\n",
        "\n",
        "def klue_bert_tokenizer(sentence, max_len):\n",
        "\n",
        "    encoded_dict = klue_tokenizer.encode_plus(\n",
        "        text = sentence,\n",
        "        max_length = max_len,\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True)\n",
        "\n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask']\n",
        "    token_type_id = encoded_dict['token_type_ids']\n",
        "\n",
        "    return input_id, attention_mask, token_type_id\n",
        "\n",
        "#  학습 데이터 생성하기\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    input_id, attention_mask, token_type_id = klue_bert_tokenizer(sentence, max_len)\n",
        "    input_ids.append(input_id)\n",
        "    attention_masks.append(attention_mask)\n",
        "    token_type_ids.append(token_type_id)\n",
        "\n",
        "# ndarray 변환\n",
        "train_input_ids = np.array(input_ids, dtype = int)\n",
        "train_attention_masks = np.array(attention_masks, dtype = int)\n",
        "train_token_type_ids = np.array(token_type_ids, dtype = int)\n",
        "train_labels = np.array(train_data.label.values, dtype = int)      # 레이블 처리\n",
        "\n",
        "# 생성된 데이터 병합\n",
        "train_inputs  = (train_input_ids, train_attention_masks, train_token_type_ids)\n",
        "\n",
        "# 모델 객체 생성\n",
        "model_name = 'klue/bert-base'\n",
        "num_labels = 2\n",
        "klue_cls_model = TFBertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                 num_labels = num_labels,\n",
        "                                                                 from_pt = True)\n",
        "klue_cls_model.summary()                   # 모델의 개요\n",
        "\n",
        "# 모델 컴파일\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-5)\n",
        "klue_cls_model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "                       optimizer = optimizer,\n",
        "                       metrics = ['accuracy'])\n",
        "\n",
        "# 학습 진행\n",
        "history = klue_cls_model.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    epochs = 1,\n",
        "    batch_size = batch_size,\n",
        "    validation_split = valid_split)\n",
        "print(history.history)\n",
        "\n",
        "# 모델 저장\n",
        "save_dir = '/content/drive/MyDrive/NLP/tf_klue_bert_naver_movie'\n",
        "cls_model.save_pretrained(save_dir)\n",
        "\n",
        "# 모델 평가\n",
        "# 평가용 데이터 전처리\n",
        "file_path = '/content/drive/MyDrive/NLP/ratings_test.txt'\n",
        "test_data = pd.read_csv(file_path, sep='\\t', quoting = 3)\n",
        "'''\n",
        "결측치 제거\n",
        "입력데이터 ndarray로 변환\n",
        "데이터 병합\n",
        "'''\n",
        "# 평가\n",
        "score = cls_model.evaluate(test_inputs, test_movie_labels, batch_size = 512)\n",
        "print(score)\n",
        "\n",
        "# 학습된 모델 부르기\n",
        "save_dir = '/content/drive/MyDrive/NLP/tf_klue_bert_naver_movie'\n",
        "loaded_klue_cls_model = TFBertForSequenceClassification.from_pretrained(save_dir)\n",
        "\n",
        "# 불러온 모델 컴파일\n",
        "loaded_klue_cls_model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "                              optimizer = optimizer,\n",
        "                              metrics = ['accuracy'])\n"
      ]
    }
  ]
}