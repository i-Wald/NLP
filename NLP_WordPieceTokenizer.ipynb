{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNri4DuJRJUOqXjp4JBk+CQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y36FpndWtqea"
      },
      "outputs": [],
      "source": [
        "# HuggingFace에서 제공하는 Tokenizer 활용\n",
        "!pip install tokenizers\n",
        "\n",
        "# Wordpiece Tokenizer 모델 불러와 객체 생성\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer_word = BertWordPieceTokenizer()\n",
        "\n",
        "# 파일 경로(영화평 모음)\n",
        "nsmc_train_text = \"/content/drive/MyDrive/NLP/nsmc_train.txt\"\n",
        "nsmc_test_text = \"/content/drive/MyDrive/NLP/nsmc_test.txt\"\n",
        "\n",
        "# WordPiece Tokenizer 학습\n",
        "tokenizer_word.train(\n",
        "    files = [nsmc_train_text, nsmc_test_text], # 학습용 데이터 입력\n",
        "    vocab_size = 10000,                        # 단어 사전의 개수\n",
        "    min_frequency = 1                          # corpus안에 단어의 최소 출현 빈도수 지정\n",
        ")\n",
        "\n",
        "# get_vocab() 함수 사용하여 생성된 단어 사전 불러오기\n",
        "vocab_wordpiece = tokenizer_word.get_vocab()\n",
        "\n",
        "print(f'단어 사전 자료형: {type(vocab_wordpiece)}')   # 단어 사전 자료형\n",
        "print(vocab_wordpiece)                                # 단어 사전 출력\n",
        "\n",
        "save_file_path = \"/content/drive/MyDrive/NLP/wordpiece\"\n",
        "\n",
        "tokenizer_word.save_model(save_file_path)      # 생성된 단어 사전 저장\n",
        "\n",
        "# vocab.txt 파일 정리\n",
        "vocab_file_path = \"/content/drive/MyDrive/NLP/wordpiece/vocab.txt\"\n",
        "\n",
        "vocab_wordpiece = []\n",
        "with open(vocab_file_path, 'r') as txt_file:\n",
        "    for vocab in txt_file:            # 개행문자(\\n) 제거\n",
        "        vocab = vocab.split('\\n')[0]\n",
        "        vocab_wordpiece.append(vocab)\n",
        "\n",
        "print(f'변환된 자료형: {type(vocab_wordpiece)}')\n",
        "print(f'단어 사전: \\n{vocab_wordpiece}')\n",
        "print(f'단어 사전의 길이: {len(vocab_wordpiece)}')\n",
        "\n",
        "# WordPiece Tokenizer 적용\n",
        "text = \"미래를 예측하는 가장 좋은 방법은 미래를 창조하는 것입니다. 이는 아브라함 링컨의 유명한 말입니다.\"\n",
        "\n",
        "encoded_wordpiece = tokenizer_word.encode(text)  # 토큰화 후 문장 숫자화 => 26개 token\n",
        "print(encoded_wordpiece)\n",
        "print(f'ids: \\n{encoded_wordpiece.ids}')\n",
        "print(f'tokens: \\n{encoded_wordpiece.tokens}')\n",
        "\n",
        "# 토큰을 원문으로 되돌리기\n",
        "decoded_wordpiece = tokenizer_word.decode(encoded_wordpiece.ids)\n",
        "print(decoded_wordpiece)"
      ]
    }
  ]
}